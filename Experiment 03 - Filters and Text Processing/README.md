# Experiment 03 – Filters and Text Processing

## Objective
To study various Linux filters and text-processing utilities such as cat, head, tail, wc, sort, uniq, grep, cut, tr, and paste.

## Commands Used
- `cat` – Display file content
- `head` – Show beginning lines
- `tail` – Show ending lines
- `wc` – Count lines, words, characters
- `sort` – Sort text
- `uniq` – Remove duplicate lines
- `grep` – Search for patterns
- `tr` – Translate characters
- `cut` – Extract specific parts of text
- `paste` – Merge lines from multiple files

## Procedure
1. Created a text file named `sample.txt` containing repeated and unsorted words.
2. Displayed file content using `cat`.
3. Viewed the first and last few lines using `head` and `tail`.
4. Counted lines, words, and characters using `wc`.
5. Sorted the data alphabetically using `sort`.
6. Removed duplicate lines using `uniq`.
7. Searched for occurrences of specific words using `grep`.
8. Converted text to uppercase using `tr`.
9. Extracted specific character positions using `cut`.
10. Combined columns using `paste`.
11. Captured the screenshot of the output and saved it as `output3.png`.

## Conclusion
This experiment demonstrated how Linux text filters and processing commands help manipulate, analyze, and transform text files efficiently.
